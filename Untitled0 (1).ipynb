{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "jbKIZvB_q6Fk"
      },
      "outputs": [],
      "source": [
        "#Installingh the required packeges\n",
        "#!pip install transformers\n",
        "#!pip install nltk\n",
        "#!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For statement 1"
      ],
      "metadata": {
        "id": "h8LjPgOJLYww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from collections import Counter\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "text = \"Definitely share your feedback in the comment section.\""
      ],
      "metadata": {
        "id": "SkVUDrnNLKLi"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import MarianTokenizer, MarianMTModel\n",
        "\n",
        "# Load the pre-trained model and tokenizer\n",
        "model_name = \"Helsinki-NLP/opus-mt-en-hi\"\n",
        "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "model = MarianMTModel.from_pretrained(model_name)\n",
        "\n",
        "def english_to_hinglish(text):\n",
        "    # Tokenize the input text\n",
        "    input_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
        "\n",
        "    # Translate to Hinglish\n",
        "    translation_ids = model.generate(input_ids)\n",
        "\n",
        "    # Decode the translated text\n",
        "    translated_text = tokenizer.decode(translation_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    return translated_text\n",
        "\n",
        "# Example usage\n",
        "#english_text = \"I had about a 30 minute demo just using this new headset\"\n",
        "hinglish_text = english_to_hinglish(text)\n",
        "print(hinglish_text)\n",
        "\n",
        "\n",
        "#pip install sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRQv5QYVxsby",
        "outputId": "c0feb65a-0604-4621-884e-c4fae23dc7d5"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "टिप्पणी खण्ड में अपनी प्रतिक्रिया को निश्‍चित ही साझा करें ।\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from collections import Counter\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Tokenize and preprocess the text\n",
        "doc = nlp(text)\n",
        "tokens = [token.text.lower() for token in doc if not token.is_stop and not token.is_punct]\n",
        "\n",
        "# Calculate word frequencies\n",
        "word_freq = Counter(tokens)\n",
        "\n",
        "# Select top keywords based on frequency (you can adjust the number)\n",
        "top_keywords = [keyword for keyword, _ in word_freq.most_common(5)]\n",
        "\n",
        "print(\"Top Keywords:\", top_keywords)\n",
        "\n",
        "# Assuming you have your list of tokens and top_keywords defined earlier\n",
        "# and an english_to_hinglish function to perform translation\n",
        "\n",
        "# Original text\n",
        "original_text = text\n",
        "\n",
        "# Tokenize the original text\n",
        "tokens = original_text.split()\n",
        "\n",
        "# Initialize an empty list to store the translated tokens\n",
        "translated_tokens = []\n",
        "\n",
        "for token in tokens:\n",
        "    if token in top_keywords:\n",
        "        # If the token is in the top_keywords list, keep it as is\n",
        "        translated_tokens.append(token)\n",
        "    else:\n",
        "        # Translate the token to Hinglish and add it to the translated_tokens list\n",
        "        translated_token = english_to_hinglish(token)\n",
        "        translated_tokens.append(translated_token)\n",
        "\n",
        "# Join the translated tokens to form the translated text\n",
        "translated_text = ' '.join(translated_tokens)\n",
        "\n",
        "print(\"Text: \",text)\n",
        "print(\"Translated text in hindi: \",hinglish_text)\n",
        "print(\"Required text: \",translated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lk3YvAHhxwYS",
        "outputId": "f523b531-f109-4313-e45a-01e63937d2fd"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top Keywords: ['definitely', 'share', 'feedback', 'comment', 'section']\n",
            "Text:  Definitely share your feedback in the comment section.\n",
            "Translated text in hindi:  टिप्पणी खण्ड में अपनी प्रतिक्रिया को निश्‍चित ही साझा करें ।\n",
            "Required text:  निश्चित रूप से share आपका feedback में वह comment खंड.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KdeWmGbq5PIa"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For statement 2"
      ],
      "metadata": {
        "id": "HK3vFI8sLhC5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#for 2nd statement\n",
        "\n",
        "\n",
        "text1 = \"So even if it's a big video, I will clearly mention all the products.\"\n",
        "\n",
        "doc = nlp(text1)\n",
        "tokens = [token.text.lower() for token in doc if not token.is_stop and not token.is_punct]\n",
        "\n",
        "# Calculate word frequencies\n",
        "word_freq = Counter(tokens)\n",
        "\n",
        "# Select top keywords based on frequency (you can adjust the number)\n",
        "top_keywords = [keyword for keyword, _ in word_freq.most_common(5)]\n",
        "\n",
        "print(\"Top Keywords:\", top_keywords)\n",
        "\n",
        "original_text = text1\n",
        "\n",
        "# Tokenize the original text\n",
        "tokens = original_text.split()\n",
        "\n",
        "# Initialize an empty list to store the translated tokens\n",
        "translated_tokens = []\n",
        "\n",
        "for token in tokens:\n",
        "    if token in top_keywords:\n",
        "        # If the token is in the top_keywords list, keep it as is\n",
        "        translated_tokens.append(token)\n",
        "    else:\n",
        "        # Translate the token to Hinglish and add it to the translated_tokens list\n",
        "        translated_token = english_to_hinglish(token)\n",
        "        translated_tokens.append(translated_token)\n",
        "\n",
        "# Join the translated tokens to form the translated text\n",
        "translated_text = ' '.join(translated_tokens)\n",
        "\n",
        "print(\"Text: \",text1)\n",
        "print(\"Translated text in hindi: \",english_to_hinglish(text1))\n",
        "print(\"Required text: \",translated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJw31cs7HaYS",
        "outputId": "d16727a7-f18b-4af5-de8d-9512619c28bf"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top Keywords: ['big', 'video', 'clearly', 'mention', 'products']\n",
            "Text:  So even if it's a big video, I will clearly mention all the products.\n",
            "Translated text in hindi:  तो यह एक बड़ा वीडियो है, तो भी मैं स्पष्ट रूप से सभी उत्पादों का उल्लेख करेंगे।\n",
            "Required text:  तो यहां तक कि यदि यह है एक big वीडियो, आई होगा clearly mention सभी वह उत्पाद.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For statement 3"
      ],
      "metadata": {
        "id": "PorsNTc_Lkpo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#for 3rd statement\n",
        "text2 = \"I was waiting for my bag.\"\n",
        "\n",
        "doc = nlp(text2)\n",
        "tokens = [token.text.lower() for token in doc if not token.is_stop and not token.is_punct]\n",
        "\n",
        "# Calculate word frequencies\n",
        "word_freq = Counter(tokens)\n",
        "\n",
        "# Select top keywords based on frequency (you can adjust the number)\n",
        "top_keywords = [keyword for keyword, _ in word_freq.most_common(5)]\n",
        "\n",
        "print(\"Top Keywords:\", top_keywords)\n",
        "\n",
        "original_text = text2\n",
        "\n",
        "# Tokenize the original text\n",
        "tokens = original_text.split()\n",
        "\n",
        "# Initialize an empty list to store the translated tokens\n",
        "translated_tokens = []\n",
        "\n",
        "for token in tokens:\n",
        "    if token in top_keywords:\n",
        "        # If the token is in the top_keywords list, keep it as is\n",
        "        translated_tokens.append(token)\n",
        "    else:\n",
        "        # Translate the token to Hinglish and add it to the translated_tokens list\n",
        "        translated_token = english_to_hinglish(token)\n",
        "        translated_tokens.append(translated_token)\n",
        "\n",
        "# Join the translated tokens to form the translated text\n",
        "translated_text = ' '.join(translated_tokens)\n",
        "\n",
        "print(\"Text: \",text2)\n",
        "print(\"Translated text in hindi: \",english_to_hinglish(text2))\n",
        "print(\"Required text: \",translated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUOskY0siZn-",
        "outputId": "e45cec27-4dcc-44f6-d54a-d79779271501"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top Keywords: ['waiting', 'bag']\n",
            "Text:  I was waiting for my bag.\n",
            "Translated text in hindi:  मैं अपने बैग के लिए इंतजार कर रहा था.\n",
            "Required text:  आई था waiting के लिए मेरा बैग.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5yiNBcyy0JDb"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "tW30kOSSIk3c"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B5yf7CNi9ExR"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NRCmaNgurMQC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zSAoijWByedd"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FSUdUqrz9PR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W2QyD_pa90yS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3tvBcgop9PuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tHktm_YHwtqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hvnpSjVh7NVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sFVTQl0Z9Px7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9Q5Ha2INuWCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nEgxZaDeo6mv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Rv75rvG6rN2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "YSWW2y1eFWrs"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n4c5LtnC9P1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c6U-zSzeGDcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5jPaaBmvHtYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iaVZWv5eH6gW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}